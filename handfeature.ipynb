{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件总行数：1229618\n",
      "\n",
      "总行数： 1229618\n",
      "                                uid                               mid  \\\n",
      "0  d38e9bed5d98110dc2489d0d1cac3c2a  7d45833d9865727a88b960b0603c19f6   \n",
      "1  fa13974743d3fe6ff40d21b872325e9e  8169f1d45051e08ef213bf1106b1225d   \n",
      "2  da534fe87e7a52777bee5c30573ed5fd  68cd0258c31c2c525f94febea2d9523b   \n",
      "3  e06a22b7e065e559a1f0bf7841a85c51  00b9f86b4915aedb7db943c54fd19d59   \n",
      "4  f9828598f9664d4e347ef2048ce17734  c7f6f66044c0c5a3330e2c5371be6824   \n",
      "\n",
      "                  time forward_count comment_count like_count  \\\n",
      "0  2015-02-23 17:41:29             0             0          0   \n",
      "1  2015-02-14 12:49:58             0             0          0   \n",
      "2  2015-03-31 13:58:06             0             0          0   \n",
      "3  2015-06-11 20:39:57             0             4          3   \n",
      "4  2015-03-10 18:02:38             0             0          0   \n",
      "\n",
      "                                             content  \n",
      "0  丽江旅游(sz002033)#股票##炒股##财经##理财##投资#推荐包赢股，盈利对半分成...  \n",
      "1  #丁辰灵的红包#挣钱是一种能力，抢红包拼的是技术。我抢到了丁辰灵 和@阚洪岩 一起发出的现金...  \n",
      "2                      淘宝网这些傻逼。。。气的劳资有火没地儿发~尼玛，你们都瞎了  \n",
      "3                                  看点不能说的，你们都懂[笑cry]  \n",
      "4                                              111多张  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('./weibo_train_data.txt', sep='\\t', header=None)\n",
    "\n",
    "# data.columns = ['uid', 'mid', 'time', 'forward_count', 'comment_count', 'like_count', 'content']\n",
    "\n",
    "with open('./WeiboData/weibo_train_data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "print(f\"文件总行数：{len(lines)}\")\n",
    "\n",
    "data = [line.strip().split('\\t') for line in lines]\n",
    "# 将列表转换为DataFrame\n",
    "data = pd.DataFrame(data, columns=['uid', 'mid', 'time', 'forward_count', 'comment_count', 'like_count', 'content'])\n",
    "\n",
    "total_rows = data.shape[0]  # 使用 shape 属性获取行数\n",
    "print(\"\\n总行数：\", total_rows)\n",
    "\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['forward_count'] = pd.to_numeric(data['forward_count'], errors='coerce')\n",
    "data['comment_count'] = pd.to_numeric(data['comment_count'], errors='coerce')\n",
    "data['like_count'] = pd.to_numeric(data['like_count'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "总行数： 1229618\n",
      "uid              object\n",
      "mid              object\n",
      "time             object\n",
      "forward_count     int64\n",
      "comment_count     int64\n",
      "like_count        int64\n",
      "content          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "total_rows = data.shape[0]  # 使用 shape 属性获取行数\n",
    "print(\"\\n总行数：\", total_rows)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 提取特征\n",
    "# 1. number_in_train\n",
    "user_counts = data['uid'].value_counts().reset_index()\n",
    "user_counts.columns = ['uid', 'number_in_train']\n",
    "\n",
    "# 2. forward_max, comment_max, like_max\n",
    "max_features = data.groupby('uid').agg({\n",
    "    'forward_count': 'max',\n",
    "    'comment_count': 'max',\n",
    "    'like_count': 'max'\n",
    "}).reset_index()\n",
    "max_features.columns = ['uid', 'forward_max', 'comment_max', 'like_max']\n",
    "\n",
    "# 3. forward_min, comment_min, like_min\n",
    "min_features = data.groupby('uid').agg({\n",
    "    'forward_count': 'min',\n",
    "    'comment_count': 'min',\n",
    "    'like_count': 'min'\n",
    "}).reset_index()\n",
    "min_features.columns = ['uid', 'forward_min', 'comment_min', 'like_min']\n",
    "\n",
    "# 4. forward_mean, comment_mean, like_mean\n",
    "mean_features = data.groupby('uid').agg({\n",
    "    'forward_count': 'mean',\n",
    "    'comment_count': 'mean',\n",
    "    'like_count': 'mean'\n",
    "}).reset_index()\n",
    "mean_features.columns = ['uid', 'forward_mean', 'comment_mean', 'like_mean']\n",
    "\n",
    "# 5. forward_judge, comment_judge, like_judge\n",
    "def calculate_judge(group):\n",
    "    return pd.Series({\n",
    "        'forward_judge': (group['forward_count'] > group['forward_count'].mean()).sum(),\n",
    "        'comment_judge': (group['comment_count'] > group['comment_count'].mean()).sum(),\n",
    "        'like_judge': (group['like_count'] > group['like_count'].mean()).sum()\n",
    "    })\n",
    "\n",
    "judge_features = data.groupby('uid').apply(calculate_judge).reset_index()\n",
    "\n",
    "# 合并所有特征\n",
    "features = user_counts.merge(max_features, on='uid', how='left')\n",
    "features = features.merge(min_features, on='uid', how='left')\n",
    "features = features.merge(mean_features, on='uid', how='left')\n",
    "features = features.merge(judge_features, on='uid', how='left')\n",
    "\n",
    "# 保存特征\n",
    "features.to_csv('./features/train_user_features.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "# 转换时间格式\n",
    "data['time'] = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 提取特征\n",
    "# 1. time_weekday\n",
    "time_weekday = data['time'].dt.dayofweek + 1  # 星期一为1，星期日为7\n",
    "\n",
    "# 2. time_weekend\n",
    "time_weekend = time_weekday.apply(lambda x: 1 if x in [6, 7] else 0)  # 星期六和星期日为周末\n",
    "\n",
    "# 3. time_hour\n",
    "time_hour = data['time'].dt.hour + 1  # 将小时转换为1到24\n",
    "\n",
    "# 4. panduan\n",
    "def judge_period(hour):\n",
    "    if 1 <= hour <= 6:\n",
    "        return 1  # 凌晨\n",
    "    elif 7 <= hour <= 12:\n",
    "        return 2  # 上午\n",
    "    elif 13 <= hour <= 18:\n",
    "        return 3  # 下午\n",
    "    else:\n",
    "        return 4  # 晚上\n",
    "\n",
    "panduan = time_hour.apply(judge_period)\n",
    "\n",
    "# 创建一个新的DataFrame来存储这些特征\n",
    "time_features = pd.DataFrame({\n",
    "    'uid': data['uid'], \n",
    "    'mid': data['mid'],  # 保留mid以便与原始数据关联\n",
    "    'time_weekday': time_weekday,\n",
    "    'time_weekend': time_weekend,\n",
    "    'time_hour': time_hour,\n",
    "    'panduan': panduan\n",
    "})\n",
    "\n",
    "# 保存结果\n",
    "time_features.to_csv('./features/train_time_features.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 处理缺失值：将缺失的content替换为空字符串\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# 初始化一个空的DataFrame来存储文本特征\n",
    "text_features = pd.DataFrame()\n",
    "\n",
    "# 提取文本特征\n",
    "# 1. length_all\n",
    "text_features['length_all'] = data['content'].apply(len)\n",
    "\n",
    "# 2. length_chinese\n",
    "text_features['length_chinese'] = data['content'].apply(lambda x: len(re.findall(r'[\\u4e00-\\u9fff]', x)))\n",
    "\n",
    "# 3. english\n",
    "text_features['english'] = data['content'].apply(lambda x: 1 if len(re.findall(r'[a-zA-Z]', x)) > len(x) / 2 else 0)\n",
    "\n",
    "# 4. non_ch\n",
    "text_features['non_ch'] = data['content'].apply(lambda x: 1 if len(re.findall(r'[\\u4e00-\\u9fff]', x)) < len(x) / 2 else 0)\n",
    "\n",
    "# 5. sharing\n",
    "text_features['sharing'] = data['content'].apply(lambda x: 1 if re.search(r'分享自|分享自|转自', x) else 0)\n",
    "\n",
    "# 6. auto\n",
    "text_features['auto'] = data['content'].apply(lambda x: 1 if re.search(r'我…了|我…了|我…了', x) and ('@' in x or 'http' in x) else 0)\n",
    "\n",
    "# 7. interaction\n",
    "text_features['interaction'] = data['content'].apply(lambda x: 1 if re.search(r'//', x) and not re.search(r'http://', x) else 0)\n",
    "\n",
    "# 8. book\n",
    "text_features['book'] = data['content'].apply(lambda x: 1 if re.search(r'《[^》]*》', x) else 0)\n",
    "\n",
    "# 9. mention\n",
    "text_features['mention'] = data['content'].apply(lambda x: 1 if '@' in x else 0)\n",
    "\n",
    "# 10. vote\n",
    "text_features['vote'] = data['content'].apply(lambda x: 1 if re.search(r'投票|投票', x) else 0)\n",
    "\n",
    "# 11. lottery\n",
    "text_features['lottery'] = data['content'].apply(lambda x: 1 if re.search(r'抽奖|抽奖', x) else 0)\n",
    "\n",
    "# 12. emoji\n",
    "text_features['emoji'] = data['content'].apply(lambda x: 1 if re.search(r'[^\\u0000-\\uFFFF]', x) else 0)\n",
    "\n",
    "# 13. video\n",
    "text_features['video'] = data['content'].apply(lambda x: 1 if re.search(r'http://v\\.weibo\\.com|http://t\\.cn', x) else 0)\n",
    "\n",
    "# 添加mid列以便与原始数据关联\n",
    "text_features['mid'] = data['mid']\n",
    "text_features['uid'] = data['uid']\n",
    "\n",
    "# 重新排列列的顺序，将mid列放在第一列\n",
    "text_features = text_features[['uid','mid', 'length_all', 'length_chinese', 'english', 'non_ch', 'sharing', 'auto', 'interaction', 'book', 'mention', 'vote', 'lottery', 'emoji', 'video']]\n",
    "\n",
    "# 保存结果\n",
    "text_features.to_csv('./features/train_text_features.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "# import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# 处理缺失值：将缺失的content替换为空字符串\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# 初始化一个空的DataFrame来存储文本特征\n",
    "text_features = pd.DataFrame()\n",
    "\n",
    "# 提取文本特征\n",
    "# 1. http\n",
    "text_features['http'] = data['content'].apply(lambda x: 1 if re.search(r'http://|https://', x) else 0)\n",
    "\n",
    "# 2. stock\n",
    "text_features['stock'] = data['content'].apply(lambda x: 1 if re.search(r'股票|股市|涨停|跌停|证券', x) else 0)\n",
    "\n",
    "# 3. app\n",
    "text_features['app'] = data['content'].apply(lambda x: 1 if re.search(r'我在#', x) else 0)\n",
    "\n",
    "# 4. title\n",
    "text_features['title'] = data['content'].apply(lambda x: 1 if re.search(r'【[^】]*】', x) else 0)\n",
    "\n",
    "# 5. ad\n",
    "text_features['ad'] = data['content'].apply(lambda x: 1 if re.search(r'广告|推广|赞助|合作', x) else 0)\n",
    "\n",
    "# 6. keywords\n",
    "# 使用jieba分词提取高频热词\n",
    "all_words = ' '.join(data['content']).split()\n",
    "word_counts = Counter(all_words)\n",
    "high_freq_words = [word for word, count in word_counts.items() if count > 100]  # 假设高频词出现次数大于100\n",
    "text_features['keywords'] = data['content'].apply(lambda x: 1 if any(word in x for word in high_freq_words) else 0)\n",
    "\n",
    "# 添加mid列以便与原始数据关联\n",
    "text_features['mid'] = data['mid']\n",
    "text_features['uid'] = data['uid']\n",
    "\n",
    "# 重新排列列的顺序，将mid列放在第一列\n",
    "text_features = text_features[['uid','mid', 'http', 'stock', 'app', 'title', 'ad', 'keywords']]\n",
    "\n",
    "# 保存结果\n",
    "text_features.to_csv('./features/train_text_features2.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件总行数：178297\n",
      "空行的行号：[]\n"
     ]
    }
   ],
   "source": [
    "with open('./WeiboData/weibo_predict_data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "print(f\"文件总行数：{len(lines)}\")\n",
    "# 检查空行\n",
    "empty_lines = [i for i, line in enumerate(lines) if line.strip() == '']\n",
    "print(f\"空行的行号：{empty_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "总行数： 178297\n"
     ]
    }
   ],
   "source": [
    "data = [line.strip().split('\\t') for line in lines]\n",
    "# 将列表转换为DataFrame\n",
    "data = pd.DataFrame(data, columns=['uid', 'mid', 'time', 'content'])\n",
    "\n",
    "total_rows = data.shape[0]  # 使用 shape 属性获取行数\n",
    "print(\"\\n总行数：\", total_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "总行数： 178297\n"
     ]
    }
   ],
   "source": [
    "total_rows = data.shape[0]  # 使用 shape 属性获取行数\n",
    "print(\"\\n总行数：\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "# 转换时间格式\n",
    "data['time'] = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 提取特征\n",
    "# 1. time_weekday\n",
    "time_weekday = data['time'].dt.dayofweek + 1  # 星期一为1，星期日为7\n",
    "\n",
    "# 2. time_weekend\n",
    "time_weekend = time_weekday.apply(lambda x: 1 if x in [6, 7] else 0)  # 星期六和星期日为周末\n",
    "\n",
    "# 3. time_hour\n",
    "time_hour = data['time'].dt.hour + 1  # 将小时转换为1到24\n",
    "\n",
    "# 4. panduan\n",
    "def judge_period(hour):\n",
    "    if 1 <= hour <= 6:\n",
    "        return 1  # 凌晨\n",
    "    elif 7 <= hour <= 12:\n",
    "        return 2  # 上午\n",
    "    elif 13 <= hour <= 18:\n",
    "        return 3  # 下午\n",
    "    else:\n",
    "        return 4  # 晚上\n",
    "\n",
    "panduan = time_hour.apply(judge_period)\n",
    "\n",
    "# 创建一个新的DataFrame来存储这些特征\n",
    "time_features = pd.DataFrame({\n",
    "    'uid': data['uid'],\n",
    "    'mid': data['mid'],  # 保留mid以便与原始数据关联\n",
    "    'time_weekday': time_weekday,\n",
    "    'time_weekend': time_weekend,\n",
    "    'time_hour': time_hour,\n",
    "    'panduan': panduan\n",
    "})\n",
    "\n",
    "# 保存结果\n",
    "time_features.to_csv('./features/predict_time_features.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 处理缺失值：将缺失的content替换为空字符串\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# 初始化一个空的DataFrame来存储文本特征\n",
    "text_features = pd.DataFrame()\n",
    "\n",
    "# 提取文本特征\n",
    "# 1. length_all\n",
    "text_features['length_all'] = data['content'].apply(len)\n",
    "\n",
    "# 2. length_chinese\n",
    "text_features['length_chinese'] = data['content'].apply(lambda x: len(re.findall(r'[\\u4e00-\\u9fff]', x)))\n",
    "\n",
    "# 3. english\n",
    "text_features['english'] = data['content'].apply(lambda x: 1 if len(re.findall(r'[a-zA-Z]', x)) > len(x) / 2 else 0)\n",
    "\n",
    "# 4. non_ch\n",
    "text_features['non_ch'] = data['content'].apply(lambda x: 1 if len(re.findall(r'[\\u4e00-\\u9fff]', x)) < len(x) / 2 else 0)\n",
    "\n",
    "# 5. sharing\n",
    "text_features['sharing'] = data['content'].apply(lambda x: 1 if re.search(r'分享自|分享自|转自', x) else 0)\n",
    "\n",
    "# 6. auto\n",
    "text_features['auto'] = data['content'].apply(lambda x: 1 if re.search(r'我…了|我…了|我…了', x) and ('@' in x or 'http' in x) else 0)\n",
    "\n",
    "# 7. interaction\n",
    "text_features['interaction'] = data['content'].apply(lambda x: 1 if re.search(r'//', x) and not re.search(r'http://', x) else 0)\n",
    "\n",
    "# 8. book\n",
    "text_features['book'] = data['content'].apply(lambda x: 1 if re.search(r'《[^》]*》', x) else 0)\n",
    "\n",
    "# 9. mention\n",
    "text_features['mention'] = data['content'].apply(lambda x: 1 if '@' in x else 0)\n",
    "\n",
    "# 10. vote\n",
    "text_features['vote'] = data['content'].apply(lambda x: 1 if re.search(r'投票|投票', x) else 0)\n",
    "\n",
    "# 11. lottery\n",
    "text_features['lottery'] = data['content'].apply(lambda x: 1 if re.search(r'抽奖|抽奖', x) else 0)\n",
    "\n",
    "# 12. emoji\n",
    "text_features['emoji'] = data['content'].apply(lambda x: 1 if re.search(r'[^\\u0000-\\uFFFF]', x) else 0)\n",
    "\n",
    "# 13. video\n",
    "text_features['video'] = data['content'].apply(lambda x: 1 if re.search(r'http://v\\.weibo\\.com|http://t\\.cn', x) else 0)\n",
    "\n",
    "# 添加mid列以便与原始数据关联\n",
    "text_features['mid'] = data['mid']\n",
    "text_features['uid'] = data['uid']\n",
    "\n",
    "# 重新排列列的顺序，将mid列放在第一列\n",
    "text_features = text_features[['uid','mid', 'length_all', 'length_chinese', 'english', 'non_ch', 'sharing', 'auto', 'interaction', 'book', 'mention', 'vote', 'lottery', 'emoji', 'video']]\n",
    "\n",
    "# 保存结果\n",
    "text_features.to_csv('./features/predict_text_features.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，结果已保存到文件中。\n"
     ]
    }
   ],
   "source": [
    "# import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# 处理缺失值：将缺失的content替换为空字符串\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# 初始化一个空的DataFrame来存储文本特征\n",
    "text_features = pd.DataFrame()\n",
    "\n",
    "# 提取文本特征\n",
    "# 1. http\n",
    "text_features['http'] = data['content'].apply(lambda x: 1 if re.search(r'http://|https://', x) else 0)\n",
    "\n",
    "# 2. stock\n",
    "text_features['stock'] = data['content'].apply(lambda x: 1 if re.search(r'股票|股市|涨停|跌停|证券', x) else 0)\n",
    "\n",
    "# 3. app\n",
    "text_features['app'] = data['content'].apply(lambda x: 1 if re.search(r'我在#', x) else 0)\n",
    "\n",
    "# 4. title\n",
    "text_features['title'] = data['content'].apply(lambda x: 1 if re.search(r'【[^】]*】', x) else 0)\n",
    "\n",
    "# 5. ad\n",
    "text_features['ad'] = data['content'].apply(lambda x: 1 if re.search(r'广告|推广|赞助|合作', x) else 0)\n",
    "\n",
    "# 6. keywords\n",
    "# 使用jieba分词提取高频热词\n",
    "all_words = ' '.join(data['content']).split()\n",
    "word_counts = Counter(all_words)\n",
    "high_freq_words = [word for word, count in word_counts.items() if count > 100]  # 假设高频词出现次数大于100\n",
    "text_features['keywords'] = data['content'].apply(lambda x: 1 if any(word in x for word in high_freq_words) else 0)\n",
    "\n",
    "# 添加mid列以便与原始数据关联\n",
    "text_features['mid'] = data['mid']\n",
    "text_features['uid'] = data['uid']\n",
    "\n",
    "# 重新排列列的顺序，将mid列放在第一列\n",
    "text_features = text_features[['uid','mid', 'http', 'stock', 'app', 'title', 'ad', 'keywords']]\n",
    "\n",
    "# 保存结果\n",
    "text_features.to_csv('./features/predict_text_features2.csv', index=False)\n",
    "\n",
    "print(\"特征提取完成，结果已保存到文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
