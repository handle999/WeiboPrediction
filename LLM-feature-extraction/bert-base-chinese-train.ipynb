{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970fc2de-29a7-4736-b52f-28c72c6db7e4",
   "metadata": {},
   "source": [
    "# 使用BERT提取文本特征，仅针对content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d936d3-7407-4205-9466-eb1fae8b5180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Highee\\.conda\\envs\\SAM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0b2297-44e9-436b-9537-724bc3e2857f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67b515-7452-4af6-a890-89b116316a29",
   "metadata": {},
   "source": [
    "## 1、读取一些内容测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb7d84-1080-40df-a37d-9031d208c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件前5行内容\n",
    "file_path = '../WeiboData/weibo_train_data.txt'\n",
    "contents = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 7:\n",
    "            contents.append(parts[6])  # 提取 content 字段\n",
    "        if len(contents) >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0801d7b8-0d81-428c-a7f5-825facf2d987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['丽江旅游(sz002033)#股票##炒股##财经##理财##投资#推荐包赢股，盈利对半分成，不算本金，群：46251412',\n",
       " '#丁辰灵的红包#挣钱是一种能力，抢红包拼的是技术。我抢到了丁辰灵 和@阚洪岩 一起发出的现金红包，幸福感爆棚！情人节，一起来和粉丝红包约个会吧╮ (￣ 3￣) ╭http://t.cn/RZDIVjf',\n",
       " '淘宝网这些傻逼。。。气的劳资有火没地儿发~尼玛，你们都瞎了',\n",
       " '看点不能说的，你们都懂[笑cry]',\n",
       " '111多张']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8c6974-b14a-41b6-81fb-090bb664d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "微博 1 内容：丽江旅游(sz002033)#股票##炒股##财经##理财#...\n",
      "特征向量前10维：(768,)\n",
      "\n",
      "微博 2 内容：#丁辰灵的红包#挣钱是一种能力，抢红包拼的是技术。我抢到了丁...\n",
      "特征向量前10维：(768,)\n",
      "\n",
      "微博 3 内容：淘宝网这些傻逼。。。气的劳资有火没地儿发~尼玛，你们都瞎了...\n",
      "特征向量前10维：(768,)\n",
      "\n",
      "微博 4 内容：看点不能说的，你们都懂[笑cry]...\n",
      "特征向量前10维：(768,)\n",
      "\n",
      "微博 5 内容：111多张...\n",
      "特征向量前10维：(768,)\n"
     ]
    }
   ],
   "source": [
    "# 提取 BERT [CLS] 特征向量\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for i, text in enumerate(contents):\n",
    "        # 编码文本\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 取 [CLS] 位置的向量（即第一个 token）\n",
    "        cls_vector = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        features.append(cls_vector)\n",
    "\n",
    "        print(f\"\\n微博 {i+1} 内容：{text[:30]}...\")\n",
    "        print(f\"特征向量前10维：{cls_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ec36a-23b0-4f70-8bf6-2f2f116e2027",
   "metadata": {},
   "source": [
    "## 2、正式提取特征，BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249edfeb-be48-4e32-b88c-18586f5ddd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "input_file = '../WeiboData/weibo_train_data.txt'\n",
    "output_file = '../features/weibo_train_bert_features.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350e0da9-91a8-4867-8071-f60bfa929b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些参数\n",
    "batch_size = 32\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bf52a4-6a86-4f8c-9b96-d7c592cb02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用batch加速\n",
    "uids, mids, contents = [], [], []\n",
    "batch = []\n",
    "\n",
    "def write_batch(writer, uids, mids, contents):\n",
    "    inputs = tokenizer(contents, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_vectors = outputs.last_hidden_state[:, 0, :]  # shape: (B, 768)\n",
    "\n",
    "    cls_vectors = cls_vectors.cpu().numpy()\n",
    "    for uid, mid, vec in zip(uids, mids, cls_vectors):\n",
    "        vec_str = ' '.join(map(str, vec))\n",
    "        writer.writerow([uid, mid, vec_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f263cfab-c60c-4058-9a21-1271ee1fdb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 178297it [00:00, 351051.41it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['uid', 'mid', 'content_feature'])\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Processing\"):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "            uid, mid, content = parts[0], parts[1], parts[6]\n",
    "            uids.append(uid)\n",
    "            mids.append(mid)\n",
    "            contents.append(content)\n",
    "\n",
    "            if len(contents) >= batch_size:\n",
    "                write_batch(writer, uids, mids, contents)\n",
    "                uids, mids, contents = [], [], []\n",
    "\n",
    "        # 处理剩余的\n",
    "        if contents:\n",
    "            write_batch(writer, uids, mids, contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
